---
title: "Machine Learning in R with Tidymodels"
author: "Jeffrey Long"
date: "12/5/2021"
output: html_document
editor_options: 
  markdown: 
    wrap: sentence
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Hadley Wickham's {tidymodels} gives a great framework for Machine Learning in R.

The following are my notes adapted from <https://www.tidymodels.org/>.

## Set Up Machine Learning Environment

```{r}
# install.packages("devtools")
# install.packages("fansi")
# install.packages("tidymodels", type = "binary")
# install.packages("broom.mixed")
# install.packages("dotwhisker")
# install.packages("hardhat")
# install.packages("rstan", repos = "https://cloud.r-project.org/", dependencies = TRUE)
# install.packages("rstanarm")
# # This is rstanarm version 2.21.1
# # - See https://mc-stan.org/rstanarm/articles/priors for changes to default priors!
# # - Default priors may change, so it's safest to specify priors, even if equivalent to the defaults.
# # - For execution on a local, multicore CPU with excess RAM we recommend calling
# #   options(mc.cores = parallel::detectCores())
# install.packages("nycflights13")
# install.packages("skimr")


library(devtools)
library(tidymodels)  # for the parsnip package, along with the rest of tidymodels

# Helper packages
library(readr)       # for importing data
library(broom.mixed) # for converting bayesian models to tidy tibbles
library(dotwhisker)  # for visualizing regression results
library(rstanarm)    # Bayesian priors
library(nycflights13)    # for flight data
library(skimr)           # for variable summaries
library(hardhat)

```

## Get Data

```{r}
urchins <-
  # Data were assembled for a tutorial 
  # at https://www.flutterbys.com.au/stats/tut/tut7.5a.html
  read_csv("https://tidymodels.org/start/models/urchins.csv") %>% 
  # Change the names to be a little more verbose
  setNames(c("food_regime", "initial_volume", "width")) %>% 
  # Factors are very helpful for modeling, so we convert one column
  mutate(food_regime = factor(food_regime, levels = c("Initial", "Low", "High")))

# # Take a look at the data
# View(urchins)
urchins
```

## Always Plot The Data

```{r}
ggplot(urchins,
       aes(x = initial_volume, 
           y = width, 
           group = food_regime, 
           col = food_regime)) + 
  geom_point() + 
  geom_smooth(method = lm, se = FALSE) +
  scale_color_viridis_d(option = "plasma", end = .7)
#> `geom_smooth()` using formula 'y ~ x'
```

## Build and Fit A Model

[List of possible engines.](https://tidymodels.github.io/parsnip/reference/linear_reg.html)

```{r}
width ~ initial_volume * food_regime
lm_mod <- 
  linear_reg() %>%   # Linear Regression Model Specification (regression)
  set_engine("lm")  # Computational engine: lm

```

From here, the model can be estimated or trained using the [fit()](https://tidymodels.github.io/parsnip/reference/fit.html) function:

```{r}
lm_fit <- 
  lm_mod %>% 
  fit(width ~ initial_volume * food_regime, data = urchins)
lm_fit
```

tidy() provides the summary results in a data frame with standard column names)

```{r}
tidy(lm_fit)
```

A dot whisker plot is a nice way to visualize the model output.

```{r}
tidy(lm_fit) %>% 
  dwplot(dot_args = list(size = 2, color = "black"),
         whisker_args = list(color = "black"),
         vline = geom_vline(xintercept = 0, colour = "grey50", linetype = 2))
```

## Using the Model to Predict

This fitted object lm_fit has the lm model output built-in, which you can access with lm_fit\$fit, but there are some benefits to using the fitted parsnip model object when it comes to predicting.

Suppose that, for a publication, it would be particularly interesting to make a plot of the mean body size for urchins that started the experiment with an initial volume of 20ml.
To create such a graph, we start with some new example data that we will make predictions for, to show in our graph:

```{r}
new_points <- expand.grid(initial_volume = 20, 
                          food_regime = c("Initial", "Low", "High"))
new_points
```

To get our predicted results, we can use the predict() function to find the mean values at 20ml.

It is also important to communicate the variability, so we also need to find the predicted confidence intervals.
If we had used lm() to fit the model directly, a few minutes of reading the documentation page for predict.lm() would explain how to do this.
However, if we decide to use a different model to estimate urchin size (spoiler: we will!), it is likely that a completely different syntax would be required.

Instead, with tidymodels, the types of predicted values are standardized so that we can use the same syntax to get these values.

First, let's generate the mean body width values:

```{r}
mean_pred <- predict(lm_fit, new_data = new_points)
mean_pred
```

When making predictions, the tidymodels convention is to always produce a tibble of results with standardized column names.
This makes it easy to combine the original data and the predictions in a usable format:

```{r}
conf_int_pred <- predict(lm_fit, 
                         new_data = new_points, 
                         type = "conf_int")
conf_int_pred

# Now combine: 
plot_data <- 
  new_points %>% 
  bind_cols(mean_pred) %>% 
  bind_cols(conf_int_pred)

# and plot:
ggplot(plot_data, aes(x = food_regime)) + 
  geom_point(aes(y = .pred)) + 
  geom_errorbar(aes(ymin = .pred_lower, 
                    ymax = .pred_upper),
                width = .2) + 
  labs(y = "urchin size")
```

## Model with A Different Engine

Every one on your team is happy with that plot except that one person who just read their first book on Bayesian analysis.
They are interested in knowing if the results would be different if the model were estimated using a Bayesian approach.
In such an analysis, a prior distribution needs to be declared for each model parameter that represents the possible values of the parameters (before being exposed to the observed data).
After some discussion, the group agrees that the priors should be bell-shaped but, since no one has any idea what the range of values should be, to take a conservative approach and make the priors wide using a Cauchy distribution (which is the same as a t-distribution with a single degree of freedom).

The documentation on the rstanarm package shows us that the stan_glm() function can be used to estimate this model, and that the function arguments that need to be specified are called prior and prior_intercept.
It turns out that linear_reg() has a stan engine.
Since these prior distribution arguments are specific to the Stan software, they are passed as arguments to parsnip::set_engine().
After that, the same exact fit() call is used:

```{r}
# set the prior distribution
prior_dist <- rstanarm::student_t(df = 1)

set.seed(123)

# make the parsnip model
bayes_mod <-   
  linear_reg() %>% 
  set_engine("stan", 
             prior_intercept = prior_dist, 
             prior = prior_dist) 

# train the model
bayes_fit <- 
  bayes_mod %>% 
  fit(width ~ initial_volume * food_regime, data = urchins)

print(bayes_fit, digits = 5)
```

To update the parameter table, the tidy() method is once again used:

```{r}
tidy(bayes_fit, conf.int = TRUE)
```

For getting predictions; we can use the same code.

```{r}
bayes_plot_data <- 
  new_points %>% 
  bind_cols(predict(bayes_fit, new_data = new_points)) %>% 
  bind_cols(predict(bayes_fit, new_data = new_points, type = "conf_int"))

ggplot(bayes_plot_data, aes(x = food_regime)) + 
  geom_point(aes(y = .pred)) + 
  geom_errorbar(aes(ymin = .pred_lower, ymax = .pred_upper), width = .2) + 
  labs(y = "urchin size") + 
  ggtitle("Bayesian model with t(1) prior distribution")
```

The [parsnip](https://parsnip.tidymodels.org/) package can work with many model types, engines, and arguments.
Check out <https://tidymodels.org/find/parsnip> to see what is available.

The extra step of defining the model using a function like linear_reg() might seem superfluous since a call to lm() is much more succinct.
However, the problem with standard modeling functions is that they don't separate what you want to do from the execution.
For example, the process of executing a formula has to happen repeatedly across model calls even when the formula does not change; we can't recycle those computations.

Also, using the tidymodels framework, we can do some interesting things by incrementally creating a model (instead of using single function call).
Model tuning with tidymodels uses the specification of the model to declare what parts of the model should be tuned.
That would be very difficult to do if linear_reg() immediately fit the model.

If you are familiar with the tidyverse, you may have noticed that our modeling code uses the magrittr pipe (%>%).
With dplyr and other tidyverse packages, the pipe works well because all of the functions take the data as the first argument.
For example:

```{r}
urchins %>% 
  group_by(food_regime) %>% 
  summarize(med_vol = median(initial_volume))
bayes_mod %>% 
  fit(width ~ initial_volume * food_regime, data = urchins)
ggplot(urchins,
       aes(initial_volume, width)) +      # returns a ggplot object 
  geom_jitter() +                         # same
  geom_smooth(method = lm, se = FALSE) +  # same                    
  labs(x = "Volume", y = "Width")
```

## Preprocess data with recipes

```{r}
set.seed(123)

flight_data <- 
  flights %>% 
  mutate(
    # Convert the arrival delay to a factor
    arr_delay = ifelse(arr_delay >= 30, "late", "on_time"),
    arr_delay = factor(arr_delay),
    # We will use the date (not date-time) in the recipe below
    date = as.Date(time_hour)
  ) %>% 
  # Include the weather data
  inner_join(weather, by = c("origin", "time_hour")) %>% 
  # Only retain the specific columns we will use
  select(dep_time, flight, origin, dest, air_time, distance, 
         carrier, date, arr_delay, time_hour) %>% 
  # Exclude missing data
  na.omit() %>% 
  # For creating models, it is better to have qualitative columns
  # encoded as factors (instead of character strings)
  mutate_if(is.character, as.factor)

flight_data %>% 
  count(arr_delay) %>% 
  mutate(prop = n/sum(n))
```

16% of the flights in this data set arrived more than 30 minutes late.

```{r}
glimpse(flight_data)
```

First, notice that the variable we created called arr_delay is a factor variable; it is important that our outcome variable for training a logistic regression model is a factor.

```{r}
flight_data %>% 
  skimr::skim(dest, carrier) 
```

Second, there are two variables that we don't want to use as predictors in our model, but that we would like to retain as identification variables that can be used to troubleshoot poorly predicted data points.
These are flight, a numeric value, and time_hour, a date-time value.

Third, there are 104 flight destinations contained in dest and 16 distinct carriers.

### Data Splitting

```{r}
# Fix the random numbers by setting the seed 
# This enables the analysis to be reproducible when random numbers are used 
set.seed(555)
# Put 3/4 of the data into the training set 
data_split <- initial_split(flight_data, prop = 3/4)

# Create data frames for the two sets:
train_data <- training(data_split)
test_data  <- testing(data_split)
```

### CREATE RECIPE AND ROLES

To get started, let's create a recipe for a simple logistic regression model.
Before training the model, we can use a recipe to create a few new predictors and conduct some preprocessing required by the model.

Let's initiate a new recipe:

```{r}
flights_rec <- 
  recipe(arr_delay ~ ., data = train_data) 
```

The recipe() function as we used it here has two arguments:

-   A formula. Any variable on the left-hand side of the tilde (\~) is considered the model outcome (here, arr_delay). On the right-hand side of the tilde are the predictors. Variables may be listed by name, or you can use the dot (.) to indicate all other variables as predictors.
-   The data. A recipe is associated with the data set used to create the model. This will typically be the training set, so data = train_data here. Naming a data set doesn't actually change the data itself; it is only used to catalog the names of the variables and their types, like factors, integers, dates, etc.

Now we can add [roles](https://tidymodels.github.io/recipes/reference/roles.html) to this recipe.
We can use the [update_role()](https://tidymodels.github.io/recipes/reference/roles.html) function to let recipes know that flight and time_hour are variables with a custom role that we called "ID" (a role can have any character value).
Whereas our formula included all variables in the training set other than arr_delay as predictors, this tells the recipe to keep these two variables but not use them as either outcomes or predictors.

```{r}
flights_rec <- 
  recipe(arr_delay ~ ., data = train_data) %>% 
  update_role(flight, time_hour, new_role = "ID") 
```

This step of adding roles to a recipe is optional; the purpose of using it here is that those two variables can be retained in the data but not included in the model.
This can be convenient when, after the model is fit, we want to investigate some poorly predicted value.
These ID columns will be available and can be used to try to understand what went wrong.

To get the current set of variables and roles, use the summary() function:

```{r}
summary(flights_rec)
```

### Create Features

Now we can start adding steps onto our recipe using the pipe operator.
Perhaps it is reasonable for the date of the flight to have an effect on the likelihood of a late arrival.
A little bit of **feature engineering** might go a long way to improving our model.
How should the date be encoded into the model?
The `date` column has an R `date` object so including that column "as is" will mean that the model will convert it to a numeric format equal to the number of days after a reference date:

```{r}
flight_data %>% 
  distinct(date) %>% 
  mutate(numeric_date = as.numeric(date)) 
```

It's possible that the numeric date variable is a good option for modeling; perhaps the model would benefit from a linear trend between the log-odds of a late arrival and the numeric date variable.
However, it might be better to add model terms derived from the date that have a better potential to be important to the model.
For example, we could derive the following meaningful features from the single date variable:

-   the day of the week,
-   the month, and
-   whether or not the date corresponds to a holiday. Let's do all three of these by adding steps to our recipe:

```{r}
flights_rec <- 
  recipe(arr_delay ~ ., data = train_data) %>% 
  update_role(flight, time_hour, new_role = "ID") %>% 
  step_date(date, features = c("dow", "month")) %>%               
  step_holiday(date, holidays = timeDate::listHolidays("US")) %>% 
  step_rm(date)
```

What do each of these steps do?
\* With `step_date()`, we created two new factor columns with the appropriate day of the week and the month.
\* With `step_holiday()`, we created a binary variable indicating whether the current date is a holiday or not.
The argument value of `timeDate::listHolidays("US")` uses the timeDate package to list the 17 standard US holidays.
\* With `step_rm()`, we remove the original date variable since we no longer want it in the model.

Next, we'll turn our attention to the variable types of our predictors.
Because we plan to train a logistic regression model, we know that predictors will ultimately need to be numeric, as opposed to factor variables.
In other words, there may be a difference in how we store our data (in factors inside a data frame), and how the underlying equations require them (a purely numeric matrix).

For factors like `dest` and `origin`, [standard practice](https://bookdown.org/max/FES/creating-dummy-variables-for-unordered-categories.html) is to convert them into dummy or indicator variables to make them numeric.
These are binary values for each level of the factor.
For example, our origin variable has values of "EWR", "JFK", and "LGA".
The standard dummy variable encoding, shown below, will create two numeric columns of the data that are 1 when the originating airport is "JFK" or "LGA" and zero otherwise, respectively.

| ORIGIN | ORIGIN_JFK | ORIGIN_LGA |
|--------|------------|------------|
| EWR    | 0          | 0          |
| JFK    | 1          | 0          |
| LGA    | 0          | 1          |

But, unlike the standard model formula methods in R, a recipe **does not** automatically create these dummy variables for you; you'll need to tell your recipe to add this step.
This is for two reasons.
First, many models do not require [numeric predictors](https://bookdown.org/max/FES/categorical-trees.html), so dummy variables may not always be preferred.
Second, recipes can also be used for purposes outside of modeling, where non-dummy versions of the variables may work better.
For example, you may want to make a table or a plot with a variable as a single factor.
For those reasons, you need to explicitly tell recipes to create dummy variables using `step_dummy()`:

```{r}
flights_rec <- 
  recipe(arr_delay ~ ., data = train_data) %>% 
  update_role(flight, time_hour, new_role = "ID") %>% 
  step_date(date, features = c("dow", "month")) %>% 
  step_holiday(date, holidays = timeDate::listHolidays("US")) %>% 
  step_rm(date) %>% 
  step_dummy(all_nominal(), -all_outcomes())
```

Here, we did something different than before: instead of applying a step to an individual variable, we used [selectors](https://tidymodels.github.io/recipes/reference/selections.html) to apply this recipe step to several variables at once.
\* The first selector, `all_nominal()`, selects all variables that are either factors or characters.
\* The second selector, `-all_outcomes()` removes any outcome variables from this recipe step.

With these two selectors together, our recipe step above translates to:

*Create dummy variables for all of the factor or character columns unless they are outcomes.*

At this stage in the recipe, this step selects the `origin`, `dest`, and `carrier` variables.
It also includes two new variables, `date_dow` and `date_month`, that were created by the earlier `step_date()`.

More generally, the recipe selectors mean that you don't always have to apply steps to individual variables one at a time.
Since a recipe knows the variable type and role of each column, they can also be selected (or dropped) using this information.

We need one final step to add to our recipe.
Since `carrier` and `dest` have some infrequently occurring factor values, it is possible that dummy variables might be created for values that don't exist in the training set.
For example, there is one destination that is only in the test set:

```{r}
test_data %>% 
  distinct(dest) %>% 
  anti_join(train_data)
```

When the recipe is applied to the training set, a column is made for LEX because the factor levels come from `flight_data` (not the training set), but this column will contain all zeros.
This is a **"zero-variance predictor"** that has no information within the column.
While some R functions will not produce an error for such predictors, it usually causes warnings and other issues.
`step_zv()` will remove columns from the data when the training set data have a single value, so it is added to the recipe *after* `step_dummy()`:

```{r}
flights_rec <- 
  recipe(arr_delay ~ ., data = train_data) %>% 
  update_role(flight, time_hour, new_role = "ID") %>% 
  step_date(date, features = c("dow", "month")) %>% 
  step_holiday(date, holidays = timeDate::listHolidays("US")) %>% 
  step_rm(date) %>% 
  step_dummy(all_nominal(), -all_outcomes()) %>% 
  step_zv(all_predictors())
```

Now we've created a *specification* of what should be done with the data.
How do we use the recipe we made?

### FIT A MODEL WITH A RECIPE

Let's use logistic regression to model the flight data.
As we saw in Build a Model, we start by building a model specification using the parsnip package:

```{r}
lr_mod <- 
  logistic_reg() %>% 
  set_engine("glm")
```

We will want to use our recipe across several steps as we train and test our model.
We will:

1.  **Process the recipe using the training set**: This involves any estimation or calculations based on the training set.
    For our recipe, the training set will be used to determine which predictors should be converted to dummy variables and which predictors will have zero-variance in the training set, and should be slated for removal.

2.  **Apply the recipe to the training set**: We create the final predictor set on the training set.

3.  **Apply the recipe to the test set**: We create the final predictor set on the test set.
    Nothing is recomputed and no information from the test set is used here; the dummy variable and zero-variance results from the training set are applied to the test set.

To simplify this process, we can use a *model workflow*, which pairs a model and recipe together.
This is a straightforward approach because different recipes are often needed for different models, so when a model and recipe are bundled, it becomes easier to train and test *workflows*.
We'll use the [workflows package](https://tidymodels.github.io/workflows/) from tidymodels to bundle our parsnip model `(lr_mod)` with our recipe `(flights_rec)`.

```{r}
flights_wflow <- 
  workflow() %>% 
  add_model(lr_mod) %>% 
  add_recipe(flights_rec)
flights_wflow
```

Now, there is a single function that can be used to prepare the recipe and train the model from the resulting predictors:

```{r}
flights_fit <- 
  flights_wflow %>% 
  fit(data = train_data)
```

This object has the finalized recipe and fitted model objects inside.
You may want to extract the model or recipe objects from the workflow.
To do this, you can use the helper functions `pull_workflow_fit()` and `pull_workflow_prepped_recipe()`.
For example, here we pull the fitted model object then use the `broom::tidy()` function to get a tidy tibble of model coefficients:

```{r}
flights_fit %>% 
  pull_workflow_fit() %>% 
  tidy()
```

### USE A TRAINED WORKFLOW TO PREDICT

Our goal was to predict whether a plane arrives more than 30 minutes late.
We have just:

1.  Built the model `(lr_mod)`,

2.  Created a preprocessing recipe `(flights_rec)`,

3.  Bundled the model and recipe `(flights_wflow)`, and

4.  Trained our workflow using a single call to `fit()`.

The next step is to use the trained workflow `(flights_fit)` to predict with the unseen test data, which we will do with a single call to `predict()`.
The `predict()` method applies the recipe to the new data, then passes them to the fitted model.

```{r}
predict(flights_fit, test_data)
```

Because our outcome variable here is a factor, the output from `predict()` returns the predicted class: late versus on_time.
But, let's say we want the predicted class probabilities for each flight instead.
To return those, we can specify type = "prob" when we use `predict()`.
We'll also bind the output with some variables from the test data and save them together:

```{r}
flights_pred <- 
  predict(flights_fit, test_data, type = "prob") %>% 
  bind_cols(test_data %>% select(arr_delay, time_hour, flight)) 

# The data look like: 
flights_pred
```

Now that we have a tibble with our predicted class probabilities, how will we evaluate the performance of our workflow?
We can see from these first few rows that our model predicted these 5 on time flights correctly because the values of `.pred_on_time` are $p > .50$.
But we also know that we have 81,454 rows total to predict.
We would like to calculate a metric that tells how well our model predicted late arrivals, compared to the true status of our outcome variable, `arr_delay`.

Let's use the area under the [ROC curve](https://bookdown.org/max/FES/measuring-performance.html#class-metrics) as our metric, computed using `roc_curve()` and `roc_auc()` from the [yardstick package](https://tidymodels.github.io/yardstick/).

To generate a ROC curve, we need the predicted class probabilities for late and on_time, which we just calculated in the code chunk above.
We can create the ROC curve with these values, using `roc_curve()` and then piping to the `autoplot()` method:

```{r}
flights_pred %>% 
  roc_curve(truth = arr_delay, .pred_late) %>% 
  autoplot()
```

Similarly, `roc_auc()` estimates the area under the curve:

```{r}
flights_pred %>% 
  roc_auc(truth = arr_delay, .pred_late)
```

Not too bad!
Next test out this workflow without this recipe.
You can use `workflows::add_formula(arr_delay ~ .)` instead of `add_recipe()` (remember to remove the identification variables first!), and see whether our recipe improved our model's ability to predict late arrivals.

### Session Info

```{r}
sessionInfo()
```
